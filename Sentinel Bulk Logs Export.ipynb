{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "This Notebook takes a KQL query and breaks it into batches that fit within the limits of the Azure Monitor API. This allows us to export more than the default 30,0000 record/64MB limits experienced when using the native interface and API directly. The export will run the batches in parallel and write the data to local disk in the format specified in the OUTPUT_FORMAT parameter (CSV or Parquet).\n",
        "### Common Use Cases\n",
        "- eDiscovery requests where a large number of rows need to exported and sent to an external party.\n",
        "- Investigations where a large number of indicators need to be exported for external analysis.\n",
        "- Compliance and external archival scenarios where certain datasets need to be stored outside of Log Analytics.\n",
        "- Data Science/Engineering scenarios where analysts need access to a large dataset in CSV or Parquet format for additional analytics outside of KQL.\n",
        "\n",
        "### Requirements and Recommended Practices\n",
        "- **Make sure you are running on a [Compute Instance](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-run-jupyter-notebooks?view=azureml-api-2#run-a-notebook-or-python-script), NOT Serverless Spark Compute (which is the default method for a new AML workspace), as that method is not currently supported.** Provision a Compute Instance with at least 4 cores. For larger datasets, you can increase the cores and memory further, just remember to update the ```JOBS``` parameter to match the number of cores as needed.\n",
        "- The [DefaultAzureCredential Class](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) is used to authenticate to the Log Analytics workspace. This should automatically authenticate the user that is launching the Notebook in AML, assuming the default SSO option is enabled when provisioning the AML compute instance and that user has access to the Log Analytics workspace. Also, ensure you press the **_\"Authenticate\"_** button if you see the _\"You need to be authenticated to the compute...\"_ banner within AML Studio. A [Managed Identity](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-compute-instance?view=azureml-api-2&tabs=azure-studio#assign-managed-identity) assigned to the AML compute instance can be used instead, but it needs to have access to the Log Analytics workspace.\n",
        "- Use the ```project``` operator in the ```QUERY``` parameter in **Step 2** to limit the amount of data being exported. This will help speed up the overall process significantly, given the current Azure Monitor API limits and the low throughput it provides. As a benchmark, it took 7 minutes to export all fields and 2.5 Million rows from the SecurityEvent table (roughly 3.2GB of data) using a 32-core AML Compute Instance. This is why being efficient in only selecting the fields needed is important for larger datasets.\n",
        "- Run each cell independently, and in order, to ensure each step runs without issue before moving on to the next. Latter steps depend on the previous steps completing successfully.\n",
        "- Review all of the parameters and their descriptions in **Step 2** to get a better sense of how to tune based on the dataset being exported.\n",
        "- Check the logs.log file within the run directory for additional troubleshooting information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1. Install Dependencies\n",
        "Run this cell to install the required Phython libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1748831971694
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install azure-monitor-query azure-identity pandas tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2. Set Parameters\n",
        "Modify the below parameters as necessary and then run the below cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1749020122548
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "#Required parameters:\n",
        "START_TIME = datetime(2025, 5, 1, tzinfo=timezone.utc) #Start time of the time range for the query.\n",
        "END_TIME = datetime(2025, 5, 31, tzinfo=timezone.utc) #End time of the time range for the query.\n",
        "QUERY = \"SecurityEvent | project TimeGenerated, Account, Computer, EventID\" #KQL query to run.\n",
        "\n",
        "#If needed, change which Log Analytics workspace to use:\n",
        "USE_DEFAULT_LAW_ID = True #If present, use the Log Analytics workspace ID that is present in the config.json file which gets created by Sentinel Notebooks.\n",
        "LAW_ID = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" #Log Analytics workspace ID to use if config.json file is not present or USE_DEFAULT_LAW_ID is set to False.\n",
        "\n",
        "#Optional parameters used for performance and output tuning:\n",
        "JOBS = 4 #Number of jobs to run in parallel. Typically, this should match the number of cores of the VM. Because the Azure Monitor API can only run 5 concurrent queries at a time, there are diminishing returns after a certain point. Any value over 64 will revert to 64.\n",
        "AUTO_BATCH = True #Attempts to automatically detect optimial batch size (time range) to use when breaking up the query.\n",
        "BATCH_SIZE = timedelta(hours=6) #If AUTO_BATCH is set to False, this batch size (time range) will be used to break up the query.\n",
        "MIN_BATCH_SIZE = timedelta(minutes=1) #If the data returned cannot fit within this time range, we skip and move to the next batch.\n",
        "OUTPUT_DIRECTORY = \"./law_export\" #Directory where results will be stored. A new directory gets created for each run.\n",
        "OUTPUT_FILE_PREFIX = \"query_results\" #Prefix used for the data files containing the query results.\n",
        "OUTPUT_FORMAT = 'CSV' #File format used to store the query results on disk. CSV or PARQUET values are supported.\n",
        "COMBINE_FORMAT = 'CSV' #File format used when combining files in Step 4. CSV or PARQUET values are supported.\n",
        "COMBINE_MAX_ROWS = 500000 #Sets the max number of rows per file when combining in Step 4.\n",
        "COMBINE_SORT = True #Sorts the data by the specified TIMESTAMP_FIELD when combining.\n",
        "TIMESTAMP_FIELD = 'TimeGenerated' #Field to use for timestamp-based batching. TimeGenerated is default. For example, _OriginalTimeGenerated can be used for data restored via Search Job.\n",
        "TG_START_TIME = datetime(2025, 5, 29, tzinfo=timezone.utc) #If using a timestamp field other than TimeGenerated, we still need to provide a time range for TimeGenerated in the request. If TIMESTAMP_FIELD is set to TimeGenerated, this parameter is ignored.\n",
        "TG_END_TIME = datetime(2025, 6, 1, tzinfo=timezone.utc) #If using a timestamp field other than TimeGenerated, we still need to provide time range for TimeGenerated in the request. If TIMESTAMP_FIELD is set to TimeGenerated, this parameter is ignored.\n",
        "TIMEOUT = 3 #Number of minutes allowed before query times out. 10 minutes is max.\n",
        "LOG_LEVEL = 'INFO' #Logging level. Supported values are the standard DEBUG, INFO, WARNING, ERROR, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3. Export Data\n",
        "Run the below cell to start the export process. Data will be written to local files in the directory specified in the OUTPUT_DIRECTORY parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1749020160761
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta, timezone\n",
        "import pandas as pd\n",
        "import time\n",
        "from azure.monitor.query import LogsQueryClient, LogsQueryStatus\n",
        "from azure.core.exceptions import HttpResponseError\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import logging\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "from multiprocessing import Pool, Manager, current_process, Queue\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "\n",
        "completed_jobs = []\n",
        "failed_jobs = []\n",
        "errors = []\n",
        "skipped_batches = []\n",
        "\n",
        "time_format: str = \"%m-%d-%Y %H-%M-%S\"\n",
        "\n",
        "class time_range_class:\n",
        "    def __init__(self, name, start_time, end_time):\n",
        "        self.name = name\n",
        "        self.start_time = start_time\n",
        "        self.end_time = end_time\n",
        "\n",
        "def get_time_ranges(start_time=datetime.now(), end_time=datetime.now() - timedelta(hours=24), number_of_ranges=5):\n",
        "    ranges = []\n",
        "    interval = (end_time - start_time) / number_of_ranges\n",
        "    delta = timedelta(microseconds=0)\n",
        "\n",
        "    index = 0\n",
        "    for i in range(number_of_ranges):\n",
        "        range_name = f\"Job{str(index)}\"\n",
        "        range_start = end_time - ((i + 1) * interval)\n",
        "        range_end = (end_time - (i * interval)) - delta\n",
        "        time_range = time_range_class(range_name, range_start, range_end)\n",
        "        ranges.append(time_range)\n",
        "        index += 1\n",
        "        delta = timedelta(microseconds=1)\n",
        "\n",
        "    return ranges\n",
        "\n",
        "def read_config_values(file_path):\n",
        "    try:\n",
        "        with open(file_path) as json_file:\n",
        "            if json_file:\n",
        "                json_config = json.load(json_file)\n",
        "                return (json_config[\"workspace_id\"])\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def write_to_file(df, export_path, prefix, format):\n",
        "    match format:\n",
        "        case 'PARQUET':\n",
        "            path = os.path.join(export_path, f\"{prefix}.parquet\")\n",
        "            df.to_parquet(path)\n",
        "        case 'CSV':\n",
        "            path = os.path.join(export_path, f\"{prefix}.csv\")\n",
        "            df.to_csv(path, index=False)    \n",
        "    \n",
        "def get_batch_size(query, law_id, start_time, end_time, timeout, timestamp_field, tg_start_time, tg_end_time):\n",
        "    batch_query_summarize = (\"| summarize NumberOfBatchesBytes = 38400000 / avg(estimate_data_size(*)), NumberOfBatchesRows = count()\"\n",
        "    \"| project NumberOfBatchesBytes = todecimal(NumberOfBatchesRows / NumberOfBatchesBytes), NumberOfBatchesRows = todecimal(NumberOfBatchesRows) / todecimal(450000)\"\n",
        "    \"| project NumberOfBatches = round(iff(NumberOfBatchesBytes > NumberOfBatchesRows, NumberOfBatchesBytes, NumberOfBatchesRows), 2)\"\n",
        "    \"| project NumberOfBatches = iif(NumberOfBatches < toreal(1), toreal(1), NumberOfBatches)\")\n",
        "  \n",
        "    if timestamp_field != 'TimeGenerated':\n",
        "        batch_query_where = (f\"{query} | where {timestamp_field} between (todatetime('{start_time}') .. todatetime('{end_time}'))\")\n",
        "        batch_query = (f\"{batch_query_where} {batch_query_summarize}\")\n",
        "        response = client.query_workspace(workspace_id=law_id, query=batch_query, timespan=(tg_start_time, tg_end_time), timeout=timeout)\n",
        "    else:\n",
        "        batch_query = (f\"{query} {batch_query_summarize}\")\n",
        "        response = client.query_workspace(workspace_id=law_id, query=batch_query, timespan=(start_time, end_time), timeout=timeout)\n",
        "\n",
        "    if response.status == LogsQueryStatus.SUCCESS:\n",
        "        data = response.tables\n",
        "    else:\n",
        "        error = response.partial_error\n",
        "        raise Exception(error.details[0][\"innererror\"])\n",
        "    for table in data:\n",
        "        df = pd.DataFrame(data=table.rows, columns=table.columns)\n",
        "        \n",
        "    return df['NumberOfBatches'].iloc[0]\n",
        "\n",
        "def define_logger(name, log_level=\"INFO\"):\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.addHandler(logging.handlers.QueueHandler(log_queue))\n",
        "    logger.setLevel(log_level)\n",
        "    logger.handlers[0].setFormatter(logging.Formatter('%(asctime)s\\t%(name)s\\t%(levelname)s\\t%(message)s', datefmt='%Y-%m-%d %H:%M:%S'))\n",
        "    return logger\n",
        "           \n",
        "def export_log_analytics_data(\n",
        "    law_id: str,\n",
        "    query: str,\n",
        "    start_time: datetime = None,\n",
        "    end_time: datetime = None,\n",
        "    batch_size: timedelta = timedelta(hours=4),\n",
        "    job_name: str = None,\n",
        "    status_queue = None,\n",
        "    log_queue = None,\n",
        "    min_batch_size: timedelta = timedelta(minutes=15),\n",
        "    client: LogsQueryClient = None,\n",
        "    export_path = '',\n",
        "    export_prefix = 'query_results',\n",
        "    auto_batch = True,\n",
        "    export_format: str = 'CSV',\n",
        "    timeout: int = 10,\n",
        "    timestamp_field = 'TimeGenerated',\n",
        "    tg_start_time: datetime = None,\n",
        "    tg_end_time: datetime = None,\n",
        "    log_level = \"INFO\",\n",
        "    delay: int = 0,\n",
        "    max_retries: int = 5,\n",
        "    export_to_file: bool = True,\n",
        "    json_depth: int = 10,\n",
        "    ):\n",
        "\n",
        "    time_range: timedelta = end_time - start_time\n",
        "    error_count: int = 0\n",
        "    initial_batch_size: timedelta = batch_size\n",
        "    batch_count: timedelta = timedelta()\n",
        "    current_count: int = 0\n",
        "    percent_complete: int = 0\n",
        "    stop_time: datetime = start_time\n",
        "    runs_without_error_count: int = 0\n",
        "    loop_done: bool = False\n",
        "    rows_returned: int = 0\n",
        "    results = []\n",
        "\n",
        "    logger = define_logger(name=f\"{current_process().name}-{job_name}\", log_level=log_level)\n",
        "    \n",
        "    logger.info(f\"Starting new job between {start_time.strftime(time_format)} and {end_time.strftime(time_format)}.\")\n",
        "\n",
        "    if auto_batch == True: \n",
        "        try:\n",
        "            batch_size = time_range / get_batch_size(query, law_id, start_time, end_time, timeout=timeout, timestamp_field=timestamp_field,tg_start_time=tg_start_time, tg_end_time=tg_end_time)\n",
        "            logger.info(f\"Calculated auto-batch size of: {batch_size}\")\n",
        "        except Exception as err:\n",
        "            logger.error(f\"Unable to auto-batch, please check your query and the log for more info. You can disable auto-batch via the AUTO_BATCH parameter if the dataset is to large to calculate. {type(err)} {err}\")\n",
        "            return ({'job_name': job_name, 'status': 'error'})\n",
        "    else:\n",
        "        logger.info(f\"Using manual batch size of: {batch_size}.\")\n",
        "\n",
        "    if batch_size > time_range: batch_size = time_range\n",
        "\n",
        "    while error_count <= max_retries:\n",
        "        try:\n",
        "            while loop_done == False:\n",
        "        \n",
        "                if batch_size < initial_batch_size and runs_without_error_count > 5:\n",
        "                    batch_size *= 2\n",
        "                    logger.info(f\"Increasing batch size to {batch_size}.\")\n",
        "                \n",
        "                start_time = end_time - batch_size\n",
        "\n",
        "                if start_time <= stop_time:\n",
        "                    start_time = stop_time\n",
        "                    batch_size = end_time - start_time\n",
        "                    loop_done = True\n",
        "                \n",
        "                logger.info(f\"Running query between {start_time.strftime(time_format)} and {end_time.strftime(time_format)}.\")\n",
        "\n",
        "                if timestamp_field != 'TimeGenerated':\n",
        "                    batch_query = (f\"{query} | where {timestamp_field} between (todatetime('{start_time}') .. todatetime('{end_time}'))\")\n",
        "                    response = client.query_workspace(workspace_id=law_id, query=batch_query, timespan=(tg_start_time, tg_end_time), timeout=timeout)\n",
        "                else:\n",
        "                    response = client.query_workspace(workspace_id=law_id, query=query, timespan=(start_time, end_time), timeout=timeout)\n",
        "                \n",
        "                if response.status == LogsQueryStatus.SUCCESS:\n",
        "                    data = response.tables\n",
        "                else:\n",
        "                    error = response.partial_error\n",
        "                    raise Exception(error.details[0][\"innererror\"])\n",
        "\n",
        "                file_prefix = f\"{export_prefix}_{start_time.strftime(time_format)}\"\n",
        "                for table in data:\n",
        "                    df = pd.DataFrame(data=table.rows, columns=table.columns)\n",
        "                    if df.shape[0] > 0: write_to_file(df, export_path, file_prefix, export_format)\n",
        "                    else: logger.info(f\"No data returned for {start_time.strftime(time_format)} and {end_time.strftime(time_format)}, skipping writing to disk.\")\n",
        "                \n",
        "                batch_count += batch_size\n",
        "                percent_complete_previous = percent_complete\n",
        "                percent_complete = round((batch_count / time_range) * 100)\n",
        "                logger.info(f\"Received {df.shape[0]} rows of data and wrote to {file_prefix}.{export_format.lower()}. Percent Complete: {percent_complete}\")\n",
        "                status_queue.put({'job_name': job_name, 'event': 'progress_update', 'message': (percent_complete - percent_complete_previous)})\n",
        "\n",
        "                rows_returned += int(df.shape[0])\n",
        "\n",
        "                runs_without_error_count += 1\n",
        "                end_time = start_time + timedelta(microseconds=-1)\n",
        "                time.sleep(delay)\n",
        "            \n",
        "            logger.info(f\"Finished exporting {rows_returned} records from Log Analytics. Percent Complete: 100\")\n",
        "            status_queue.put({'job_name': job_name, 'event': 'progress_update', 'message': (100 - percent_complete)})\n",
        "            close_logger(logger)\n",
        "\n",
        "            return ({'job_name': job_name, 'status': 'success', 'rows_returned_total': rows_returned})\n",
        "        except Exception as err:\n",
        "            if \"Response ended prematurely\" in str(err):\n",
        "                logger.warning(f\"Response ended prematurely, retrying.\")\n",
        "                logger.debug(f\"{type(err)} {err}\")\n",
        "            elif (\"A recognition error occurred in the query\") in str(err) or \"A semantic error occurred\" in str(err) or \"The requested path does not exist\" in str(err):\n",
        "                logger.error(f\"There is likely an error in the query or the workspace ID. {type(err)} {err}\")\n",
        "                return ({'job_name': job_name, 'status': 'error'})\n",
        "            elif (\"Maximum response size of 100000000 bytes exceeded\" in str(err) \n",
        "            or 'The results of this query exceed the set limit of 64000000 bytes' in str(err) \n",
        "            or 'The results of this query exceed the set limit of 500000 records' in str(err)):\n",
        "                runs_without_error_count = 0\n",
        "                if batch_size == min_batch_size:\n",
        "                    status_queue.put({'job_name': job_name, 'event': 'skipped_batch', 'message': f\"{start_time.strftime(time_format)} - {end_time.strftime(time_format)}\"})\n",
        "                    logger.warning(f\"Results cannot be returned in the specified minimum batch size. Skipping batch between {start_time.strftime(time_format)} and {end_time.strftime(time_format)}.\")\n",
        "                    logger.debug(f\"{type(err)} {err}\")\n",
        "                    batch_count += batch_size\n",
        "                    end_time = start_time + timedelta(microseconds=-1)\n",
        "                    loop_done = False\n",
        "                else:\n",
        "                    batch_size = batch_size / 2\n",
        "                    if batch_size < min_batch_size:\n",
        "                        batch_size = min_batch_size\n",
        "                    logger.info(f\"Request was too large, reduced batch size to: {batch_size}.\")\n",
        "                    logger.debug(f\"{type(err)} {err}\")\n",
        "                    loop_done = False\n",
        "            else:\n",
        "                logger.error(f\"Unhandled Error: {type(err)} {err}\")\n",
        "                error_count += 1\n",
        "                if error_count > max_retries:\n",
        "                    logger.error(\"Max number of retries reached, exiting.\")\n",
        "                    return ({'job_name': job_name, 'status': 'error'})\n",
        "\n",
        "def log_result(result):\n",
        "    global completed_jobs\n",
        "    global failed_jobs\n",
        "    if result['status'] == 'success':\n",
        "        completed_jobs.append(result)\n",
        "    else:\n",
        "        logger.error(f\"{result['job_name']} has failed. Please check log file for details.\")\n",
        "        failed_jobs.append(result)\n",
        "\n",
        "def log_error(error):\n",
        "    logger.error(error)\n",
        "\n",
        "def logger_process(queue, job_directory, log_level):\n",
        "    message: logging.LogRecord\n",
        "    logger = logging.getLogger('logger_process')\n",
        "    logger.addHandler(logging.FileHandler(f\"{job_directory}/logs.log\"))\n",
        "    logger.handlers[0].setLevel(log_level)\n",
        "    while True:\n",
        "        message = queue.get()\n",
        "        if message is None:\n",
        "            break\n",
        "        if message.levelno >= 40:\n",
        "            status_queue.put({'job_name': message.processName, 'event': 'error', 'message': message})\n",
        "        logger.handle(message)\n",
        "    close_logger(logger)\n",
        "\n",
        "def close_logger(logger):\n",
        "    for handler in logger.handlers:\n",
        "        logger.removeHandler(handler)\n",
        "        handler.close()\n",
        "\n",
        "def get_workspace():\n",
        "    workspace_id = read_config_values('config.json')\n",
        "\n",
        "    if workspace_id == None or USE_DEFAULT_LAW_ID == False:\n",
        "        if LAW_ID != \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\":\n",
        "            law_id = LAW_ID\n",
        "        else:\n",
        "            raise Exception(\"Please specify a valid Log Analyics workspace ID in the Parameters cell.\")\n",
        "    else:\n",
        "        law_id = workspace_id\n",
        "\n",
        "    return law_id\n",
        "\n",
        "def make_directory():\n",
        "    if not os.path.exists(OUTPUT_DIRECTORY): os.makedirs(OUTPUT_DIRECTORY)\n",
        "    job_directory = f\"{OUTPUT_DIRECTORY}/{datetime.now().strftime(time_format)}\"\n",
        "    os.mkdir(job_directory)\n",
        "    \n",
        "    return job_directory\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "job_directory = make_directory()\n",
        "if JOBS > 64: JOBS = 64\n",
        "ranges = get_time_ranges(start_time=START_TIME, end_time=END_TIME, number_of_ranges=JOBS )\n",
        "law_id = get_workspace()    \n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "client = LogsQueryClient(credential)\n",
        "\n",
        "pbar = tqdm(total=JOBS * 100, leave=True, position=0, desc=f\"Splitting query into {JOBS} jobs for parallel processing\", postfix={'Errors': 0, 'Skipped Batches': 0})\n",
        "\n",
        "with Manager() as manager:\n",
        "    status_queue = manager.Queue()\n",
        "    log_queue = manager.Queue()\n",
        "    last_queue_time = datetime.now()\n",
        "\n",
        "    with Pool() as pool:\n",
        "        logger = define_logger(name=current_process().name, log_level=LOG_LEVEL)\n",
        "        logger.info(f\"Starting logger process.\")\n",
        "        pool_logger_results = pool.apply_async(logger_process, args=(log_queue, job_directory, LOG_LEVEL), error_callback=log_error)\n",
        "        logger.info(f\"Starting {len(ranges)} query jobs between {START_TIME.strftime(time_format)} and {END_TIME.strftime(time_format)}.\")\n",
        "        pool_export_results = [pool.apply_async(export_log_analytics_data, [law_id, QUERY, i.start_time, i.end_time, BATCH_SIZE, i.name, status_queue, log_queue, MIN_BATCH_SIZE, client, job_directory, OUTPUT_FILE_PREFIX, AUTO_BATCH, OUTPUT_FORMAT, TIMEOUT, TIMESTAMP_FIELD, TG_START_TIME, TG_END_TIME, LOG_LEVEL], callback=log_result, error_callback=log_error) for  i in ranges]\n",
        "        while (len(completed_jobs) + len(failed_jobs)) < JOBS or not status_queue.empty():\n",
        "            if not status_queue.empty():\n",
        "                item = status_queue.get()\n",
        "                last_queue_time = datetime.now()\n",
        "                match item['event']:\n",
        "                    case 'progress_update': \n",
        "                        pbar.update(item['message'])\n",
        "                    case 'skipped_batch': \n",
        "                        skipped_batches.append(item)\n",
        "                        pbar.set_postfix(ordered_dict={'\\033[91mErrors': len(errors), 'Skipped Batches': len(skipped_batches)})\n",
        "                    case 'error':\n",
        "                        errors.append(item)\n",
        "                        pbar.set_postfix(ordered_dict={'\\033[91mErrors': len(errors), 'Skipped Batches': len(skipped_batches)})\n",
        "            if datetime.now() - last_queue_time > timedelta(minutes=TIMEOUT):\n",
        "                log_message = f\"No input received from running job(s) for more than {TIMEOUT} minutes, check log for errors. Exiting.\"\n",
        "                logger.error(log_message)\n",
        "                pbar.set_description(log_message)\n",
        "                break\n",
        "        if len(completed_jobs) > 0:\n",
        "            log_message = f\"Completed export of {sum([item['rows_returned_total'] for item in completed_jobs])} records to {job_directory}/.\"\n",
        "            logger.info(log_message)\n",
        "            pbar.set_description(log_message)\n",
        "        else:\n",
        "            pbar.set_description_str(f\"No jobs completed successfully. Please check log file in {job_directory} for details.\")\n",
        "        log_queue.put(None)\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "        close_logger(logger)\n",
        "\n",
        "time.sleep(2)\n",
        "\n",
        "pbar.clear()\n",
        "pbar.close()\n",
        "\n",
        "time.sleep(2)\n",
        "\n",
        "if skipped_batches:\n",
        "    print(\"\\nThe below batches were skipped. Try lowering the MIN_BATCH_SIZE parameter or reduce the size of the dataset. Review log file for more details.\\n\")\n",
        "    for item in skipped_batches: print(item['message'])\n",
        "   \n",
        "if errors:\n",
        "    print(\"\\nThe below errors were encountered. Review log file for more details.\\n\")\n",
        "    for item in errors: print(item['message'].message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 4. Combine Files (Optional)\n",
        "Merges rows from individual data files into one or more files based on the COMBINE_MAX_ROWS parameter. Ensure you have enough memory allocated in your compute instance to account for the size of the exported dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1749020249190
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def combine_files(files, output_prefix, job_directory, output_format, combine_format, sort_column, sort, max_rows=50000):\n",
        "    export_path = f'{job_directory}/combined'\n",
        "    if not os.path.exists(export_path): os.mkdir(export_path)\n",
        "        \n",
        "    if output_format.lower() == 'csv':\n",
        "        df = [pd.read_csv(file, low_memory=False) for file in tqdm(files, position=0, desc=\"Reading files\")]\n",
        "    if output_format.lower() == 'parquet':\n",
        "        df = [pd.read_parquet(file) for file in tqdm(files, position=0, desc=\"Reading files\")]\n",
        "    file_index = 1\n",
        "    row_index = 0\n",
        "    \n",
        "    pbar_combine = tqdm(leave=True, position=0, desc=\"Concatenating files, please wait\")\n",
        "    df = pd.concat(df, ignore_index=True)\n",
        "    if sort: df.sort_values(by=[sort_column], inplace=True)\n",
        "    total_rows = df.shape[0]\n",
        "    pbar_combine.total = total_rows\n",
        "    while row_index < total_rows:\n",
        "        pbar_combine.set_description(f'Writing max of {max_rows} rows to each file')\n",
        "        if combine_format.lower() == 'csv':\n",
        "            df[row_index:(row_index + max_rows)].to_csv(f'{export_path}/{output_prefix}_combined{file_index}.csv', index=False)\n",
        "        if combine_format.lower() == 'parquet':\n",
        "            df[row_index:(row_index + max_rows)].to_parquet(f'{export_path}/{output_prefix}_combined{file_index}.parquet', index=False)\n",
        "        row_index += (max_rows)\n",
        "        file_index += 1\n",
        "        \n",
        "        if row_index > total_rows:\n",
        "            pbar_combine.update(total_rows - (row_index - (max_rows)))\n",
        "            pbar_combine.set_description(f'Completed combining {total_rows} rows into {file_index - 1} file(s), located in {export_path}/.')\n",
        "        else:\n",
        "            pbar_combine.update(max_rows) \n",
        "    pbar_combine.clear()\n",
        "    pbar_combine.close()\n",
        "\n",
        "clear_output(wait=True)\n",
        "if OUTPUT_FORMAT.lower() in ('csv', 'parquet'):\n",
        "    files = glob.glob(job_directory + '/*.{}'.format(OUTPUT_FORMAT.lower()))\n",
        "    if TIMESTAMP_FIELD == 'TimeGenerated': \n",
        "        sort_column = TIMESTAMP_FIELD\n",
        "    else:\n",
        "        sort_column = TIMESTAMP_FIELD\n",
        "    if files: combine_files(files=files, output_prefix=OUTPUT_FILE_PREFIX, output_format=OUTPUT_FORMAT, job_directory=job_directory, combine_format=COMBINE_FORMAT, max_rows=COMBINE_MAX_ROWS, sort_column=sort_column, sort=COMBINE_SORT)\n",
        "    else: print(\"No files found.\")\n",
        "else: print(\"Please set OUTPUT_FORMAT parameter value to either CSV or PARQUET. Exiting.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5. Delete Data (Optional)\n",
        "Run the below cell to DELETE all run data including logs and data files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1749020231074
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from IPython.display import clear_output\n",
        "\n",
        "confirmation = input(f\"Are you sure you want to delete the directory {OUTPUT_DIRECTORY} and all of its contents? (Y/N)\")\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "if confirmation.lower() in ('y', 'yes'):\n",
        "    try:\n",
        "        print(\"Deleting data...\")\n",
        "        shutil.rmtree(OUTPUT_DIRECTORY)\n",
        "        print(\"Data has been deleted.\")\n",
        "    except Exception as err:\n",
        "        print(f\"Error deleting data: {err}\")\n",
        "else:\n",
        "    print('Operation has been aborted.')"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
